{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RISHU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Custom defaultdict implementation\n",
    "class MyDefaultDict(dict):\n",
    "    def __init__(self, default_factory, *args, **kwargs):\n",
    "        self.default_factory = default_factory\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        if self.default_factory is None:\n",
    "            raise KeyError(key)\n",
    "        else:\n",
    "            self[key] = self.default_factory()\n",
    "            return self[key]\n",
    "\n",
    "# Read training and test data from CSV files\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Preprocess data\n",
    "train_sentences = train_data['untagged_sentence'].tolist()\n",
    "train_tags = train_data['tagged_sentence'].tolist()\n",
    "\n",
    "# Function to convert tagged sentences to NLTK format\n",
    "def convert_to_nltk_format(tagged_sentence):\n",
    "    # Extracting the tagged words and tags from the input\n",
    "    tagged_words = eval(tagged_sentence)\n",
    "    words = [word for word, tag in tagged_words]\n",
    "    tags = [tag for word, tag in tagged_words]\n",
    "    # Combining words and tags into a list of tuples\n",
    "    nltk_format = list(zip(words, tags))\n",
    "    return nltk_format\n",
    "\n",
    "# Convert all tagged sentences to NLTK format\n",
    "nltk_train_tags = [convert_to_nltk_format(tagged_sentence) for tagged_sentence in train_tags]\n",
    "\n",
    "random.seed(1234)\n",
    "train_set = nltk_train_tags\n",
    "\n",
    "# Prepare the features for the classifier\n",
    "def features(sentence, index):\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "\n",
    "# Prepare the dataset\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for tagged_sentence in nltk_train_tags:\n",
    "    untagged_sentence = [w for w, t in tagged_sentence]\n",
    "    for index in range(len(untagged_sentence)):\n",
    "        X_train.append(features(untagged_sentence, index))\n",
    "        y_train.append(tagged_sentence[index][1])\n",
    "\n",
    "# Convert the dictionary of features to a feature vector\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Train the MEMM using logistic regression\n",
    "clf = LogisticRegression(solver='saga', random_state=1234)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the MEMM for prediction\n",
    "def predict(sentence):\n",
    "    tagged_sentence = []\n",
    "    tags = clf.classes_\n",
    "    for index in range(len(sentence)):\n",
    "        features_transformed = vectorizer.transform([features(sentence, index)])\n",
    "        probabilities = clf.predict_proba(features_transformed)[0]\n",
    "        max_index = np.argmax(probabilities)\n",
    "        tagged_sentence.append((sentence[index], tags[max_index]))\n",
    "    return tagged_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [46:16,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load your data\n",
    "test_df = pd.read_csv('test_small.csv')\n",
    "\n",
    "# Initialize a list to store tagged sentences\n",
    "tagged_sentences = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in tqdm(test_df.iterrows()):\n",
    "    # Extract the untagged sentence and its corresponding id\n",
    "    untagged_sentence = ast.literal_eval(row['untagged_sentence'])\n",
    "    sentence_id = row['id']\n",
    "    \n",
    "    # Use the Viterbi algorithm to tag each word in the sentence\n",
    "    tagged_sentence = predict(untagged_sentence)\n",
    "    \n",
    "    # Combine the id and tagged sentence into a string and append to the list\n",
    "    tagged_sentences.append({'id': sentence_id, 'tagged_sentence': tagged_sentence})\n",
    "    \n",
    "\n",
    "# Convert the list of tagged sentences to a DataFrame\n",
    "tagged_df = pd.DataFrame(tagged_sentences)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "tagged_df.to_csv('samples.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Twitter', 'NN'), ('is', 'BE'), ('the', 'AT'), ('best', 'JJ'), ('networking', 'VB'), ('social', 'JJ'), ('site', 'NN'), ('.', '.'), ('Man', 'NN'), ('is', 'BE'), ('a', 'AT'), ('social', 'JJ'), ('animal', 'NN'), ('.', '.'), ('Data', 'NN'), ('science', 'NN'), ('is', 'BE'), ('an', 'AT'), ('emerging', 'VB'), ('field', 'NN'), ('.', '.'), ('Data', 'NN'), ('science', 'NN'), ('jobs', 'NN'), ('are', 'BE'), ('high', 'JJ'), ('in', 'IN'), ('demand', 'NN'), ('.', '.')]\n",
      "0.879969596862793\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "## Testing\n",
    "sentence_test = 'Twitter is the best networking social site. Man is a social animal. Data science is an emerging field. Data science jobs are high in demand.'\n",
    "words = word_tokenize(sentence_test)\n",
    "\n",
    "start = time.time()\n",
    "tagged_seq = predict(words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(tagged_seq)\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
